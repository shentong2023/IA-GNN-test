nohup: ignoring input
Namespace(batch_size=100, dataset='diginetica', emb_size=100, epoch=30, epsilon=0.85, evaluate_k=[10, 20], l2=1e-05, lr=0.001, lr_dc=0.1, lr_dc_step=10, n_heads=3, n_intentions=3, patience=10, temp=0.1)
--------------------------------------
epoch 0
lr: 0.001
start training... 2022-04-10 16:41:58.604723
	 train_loss : 46790.250
this_epoch----
recall@10:0.2334	 mrr@10:0.0921
recall@20:0.3321	 mrr@20:0.0989
best_result----
recall@10:0.2334	 mrr@10:0.0921	 epoch:0,0
recall@20:0.3321	 mrr@20:0.0989	 epoch:0,0
--------------------------------------
epoch 1
lr: 0.001
start training... 2022-04-10 16:50:21.294387
	 train_loss : 36426.949
this_epoch----
recall@10:0.2657	 mrr@10:0.1042
recall@20:0.3779	 mrr@20:0.1119
best_result----
recall@10:0.2657	 mrr@10:0.1042	 epoch:1,1
recall@20:0.3779	 mrr@20:0.1119	 epoch:1,1
--------------------------------------
epoch 2
lr: 0.001
start training... 2022-04-10 16:58:29.227123
	 train_loss : 32706.912
this_epoch----
recall@10:0.2725	 mrr@10:0.1088
recall@20:0.3883	 mrr@20:0.1168
best_result----
recall@10:0.2725	 mrr@10:0.1088	 epoch:2,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 3
lr: 0.001
start training... 2022-04-10 17:07:07.684517
	 train_loss : 30764.914
this_epoch----
recall@10:0.2724	 mrr@10:0.1085
recall@20:0.3881	 mrr@20:0.1164
best_result----
recall@10:0.2725	 mrr@10:0.1088	 epoch:2,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 4
lr: 0.001
start training... 2022-04-10 17:15:46.394534
	 train_loss : 29442.982
this_epoch----
recall@10:0.2730	 mrr@10:0.1084
recall@20:0.3856	 mrr@20:0.1161
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 5
lr: 0.001
start training... 2022-04-10 17:24:24.192277
	 train_loss : 28454.820
this_epoch----
recall@10:0.2674	 mrr@10:0.1054
recall@20:0.3824	 mrr@20:0.1134
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 6
lr: 0.001
start training... 2022-04-10 17:33:02.779816
	 train_loss : 27646.602
this_epoch----
recall@10:0.2640	 mrr@10:0.1034
recall@20:0.3763	 mrr@20:0.1111
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 7
lr: 0.001
start training... 2022-04-10 17:41:31.362009
	 train_loss : 26975.166
this_epoch----
recall@10:0.2588	 mrr@10:0.1011
recall@20:0.3707	 mrr@20:0.1088
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 8
lr: 0.001
start training... 2022-04-10 17:49:36.048781
	 train_loss : 26379.475
this_epoch----
recall@10:0.2586	 mrr@10:0.1004
recall@20:0.3667	 mrr@20:0.1078
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 9
lr: 0.001
start training... 2022-04-10 17:58:12.050187
	 train_loss : 25834.689
this_epoch----
recall@10:0.2535	 mrr@10:0.0992
recall@20:0.3596	 mrr@20:0.1065
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 10
lr: 0.0001
start training... 2022-04-10 18:06:45.029371
	 train_loss : 21658.414
this_epoch----
recall@10:0.2702	 mrr@10:0.1073
recall@20:0.3783	 mrr@20:0.1147
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 11
lr: 0.0001
start training... 2022-04-10 18:15:20.453319
	 train_loss : 20731.682
this_epoch----
recall@10:0.2696	 mrr@10:0.1076
recall@20:0.3780	 mrr@20:0.1151
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 12
lr: 0.0001
start training... 2022-04-10 18:23:52.250288
	 train_loss : 20347.635
this_epoch----
recall@10:0.2690	 mrr@10:0.1061
recall@20:0.3773	 mrr@20:0.1136
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 13
lr: 0.0001
start training... 2022-04-10 18:32:19.273658
	 train_loss : 20077.867
this_epoch----
recall@10:0.2677	 mrr@10:0.1054
recall@20:0.3756	 mrr@20:0.1128
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
--------------------------------------
epoch 14
lr: 0.0001
start training... 2022-04-10 18:38:15.445789
	 train_loss : 19858.670
this_epoch----
recall@10:0.2657	 mrr@10:0.1046
recall@20:0.3738	 mrr@20:0.1121
best_result----
recall@10:0.2730	 mrr@10:0.1088	 epoch:4,2
recall@20:0.3883	 mrr@20:0.1168	 epoch:2,2
Done
